# -*- coding: utf-8 -*-
"""1_toxic_hate_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1st7iVOl9gGdSBP6dciMHgyxFzFgROhJ-
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import seaborn as sns
from matplotlib import style
style.use("ggplot")
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

from google.colab import drive
drive.mount("/content/drive")

data = pd.read_csv("/content/drive/MyDrive/NLP/toxic_data/train.csv")

data.info()

data.head()  #shape , text je yg tkde parentheses

data.describe()

data.label.value_counts()

data.label.sum()

print(data.tweet.iloc[0], "\n")   #comma or plusss
print(data["tweet"].iloc[1])   #first is column, then iloc means which rowsssss ,similar mcm array, array row then column

def data_preprocessing(text):
  text =text.lower()
  text = re.sub(r"https\S+|www\S+http\S+", '', text, flags = re.MULTILINE)
  text = re.sub(r'\@w+|\#','', text)
  text = re.sub(r'[^\w\s]','',text)
  text = re.sub(r'รฐ','',text)
  text = word_tokenize(text)
  filtered = [word for word in text if word not in stop_words ]
  filter = " ".join(filtered)
  return filter

import nltk
nltk.download('punkt')

data.tweet = data.tweet.apply(data_preprocessing)  #apply on the dataset, call the function on the dataset
data

data = data.drop_duplicates("tweet")
data.shape

import nltk
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
def lemmatizing(data):   #so same thing for lemmatization, need to separate the words in the sentence, lemma using list , then join back using " ".join
    tweet = word_tokenize(data)
    tweet = [lemmatizer.lemmatize(word) for word in tweet]
    tweet = " ".join(tweet)
    return tweet

data["tweet"] = data["tweet"].apply(lambda x : lemmatizing(x))  #lambda will take the text in each row of the column and enter the function
print(data.tweet.iloc[0], "\n")   #comma or plusss
print(data["tweet"].iloc[1])

#sum() is to calculate the numeric sum of array or list, value_counts is to create the frequency distribution in a categorical values

data.tweet

data["label"].value_counts()

fig = plt.figure(figsize = (5,5))
sns.countplot(x="label", data = data)

fig = plt.figure(figsize=(7,7))
colors = ("red","green")
wp = {"linewidth" : 2,"edgecolor" : "black"}
tags = data["label"].value_counts()
explode =(0.1,0.1)
tags.plot(kind='pie',autopct = '%1.1f%%', shadow=True, colors = colors, startangle =90,
         wedgeprops = wp, explode = explode, label='')
plt.title('Distribution of sentiments')

non_toxic = data[data.label==0]  #to find the specific  dataset label number list based on their categorical values
non_toxic

testttt = data.query("id==1")
testttt

text =" ".join([word for word in non_toxic.tweet])
plt.figure(figsize = (7,7), facecolor=None)
wordcloud = WordCloud(max_words = 500, width = 1600, height= 800).generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most frequent words in non hate tweets', fontsize = 19)
plt.show()

vect = TfidfVectorizer(ngram_range=(1,3)).fit(data["tweet"])
feature_names = vect.get_feature_names_out()
feature_names

x = data["tweet"]
y = data["label"]

x=vect.transform(x)   #after vectorizer than only transform
x_train, x_test, y_train, y_test =train_test_split(x, y , train_size=0.8,test_size = 0.2, random_state=42)

test = data[["tweet","label"]] #kena double square bracket , parentheses, semi colom, curly, colon, comma, quote
test

import pickle
logreg = LogisticRegression()
logreg.fit(x_train,y_train)
y_predict = logreg.predict(x_test)
accuracyscore = accuracy_score(y_predict,y_test)

with open("/content/drive/MyDrive/NLP/toxic_data/toxicmode.pkl","wb") as file:
  pickle.dump(logreg,file)

with open("/content/drive/MyDrive/NLP/toxic_data/vectoriserfortoxic.pkl","wb") as file:
  pickle.dump(vect,file)

import joblib
import pickle
with open("/content/drive/MyDrive/NLP/toxic_data/toxicmode.pkl","rb") as file:
  model = pickle.load(file)

with open("/content/drive/MyDrive/NLP/toxic_data/vectoriserfortoxic.pkl","rb") as file:
  vect = pickle.load(file)

y = model.predict(x_test)
accuracy = accuracy_score(y, y_test)
accuracy

text = ["i hate you"]
x=vect.transform(text)
tt= model.predict(x)
tt

print(round(accuracyscore*100,2))
print("{:.2f}".format(accuracyscore))   #format, can be used for string, float, int,  .2f means 2 float decimal places
print(f"{accuracyscore:.2f}")
hello = "hello"
age = 23
print("i am {} and im {}".format(hello,age))   #either {}.format or f"{}"
print(f"i am {hello} and im {age}")

list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling',
         'driving', 'died', 'tried', 'feet']
for i in list1:
  print(i + " ->" + lemmatizer.lemmatize(i))


#stemming will reduce the word into base form, but inconsistency, lemmatizing will reduce the word based on their context, both reduce the suffix and prefix
#sent_tokenize = separate into sentence, word_tokenize = separate into words
#